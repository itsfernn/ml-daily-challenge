{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAEoYJL-6VqJ"
   },
   "source": [
    "# LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA, short for **Low-Rank Adaptation**, is one of the most popular *parameter-efficient fine-tuning* (PEFT) methods. It was first proposed by **[Hu et al., 2021](https://arxiv.org/pdf/2106.09685)**, and has become a go-to technique when adapting large pretrained models to new tasks.\n",
    "\n",
    "Why do we need PEFT methods in the first place?\n",
    "Finetuning large language models in the traditional way—updating all of their billions of parameters—is simply too expensive in terms of compute, memory, and storage. Researchers realized that we don’t actually need to change every parameter of a pretrained model to make it useful for new tasks.\n",
    "\n",
    "This idea was first explored in **[Houlsby et al., 2019](https://arxiv.org/pdf/1902.00751)** with their influential *adapter* paper. They showed that instead of retraining the entire model, we can insert small trainable modules (adapters) that steer the model’s behavior while leaving the pretrained weights frozen. This kicked off the PEFT era.\n",
    "\n",
    "---\n",
    "\n",
    "## Why LoRA?\n",
    "\n",
    "LoRA improves upon adapters in a very elegant way. Instead of adding entire new modules, LoRA injects low-rank matrices into existing weight updates. This has several advantages:\n",
    "\n",
    "* **Fewer parameters**: LoRA can reduce the number of trainable parameters even further compared to adapters.\n",
    "* **No extra latency**: Since LoRA modifies existing weight matrices directly, there’s no runtime penalty. It acts more like a *patch* than a separate component.\n",
    "* **Reversible updates**: Applying LoRA is simple—you add the low-rank “patch” to the original weights. To restore the original model, you just remove it.\n",
    "\n",
    "This efficiency and simplicity explain why LoRA has become so widely adopted in the LLM community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oueNNRs189De"
   },
   "source": [
    "## What is LoRA?\n",
    "\n",
    "![lora](diagram.png)\n",
    "\n",
    "LoRA is deceptively simple to understand. The key idea is this: **we don’t need to change all the parameters of a pretrained model**, because the model already encodes a huge amount of knowledge. Instead, we can apply a *small patch* that shifts the model’s behavior in the desired direction.\n",
    "\n",
    "Every transformer model is built from components such as **attention blocks** and **MLPs**. If we zoom in, these components are nothing more than weight matrices, usually denoted as $W_0$. A straightforward fine-tuning update would be:\n",
    "\n",
    "$$\n",
    "W = W_0 + \\Delta W\n",
    "$$\n",
    "\n",
    "But if we allow $\\Delta W$ to be a full matrix for *all* weights in the model, we are essentially just fine-tuning again — which defeats the purpose. LoRA introduces two crucial insights to make this efficient:\n",
    "\n",
    "1. **Target only specific matrices**\n",
    "   We don’t need to apply LoRA to every weight matrix. In the original paper, after experimenting with different choices, Hu et al. focused on the **query ($W_q$) and value ($W_v$) projection matrices** in the attention block. This already gave strong performance while reducing cost.\n",
    "\n",
    "2. **Use low-rank factorization**\n",
    "   Instead of learning a full $d \\times d$ matrix for $\\Delta W$, LoRA learns two much smaller matrices:\n",
    "\n",
    "   * a **down-projection** of size $d \\times r$\n",
    "   * an **up-projection** of size $r \\times d$\n",
    "\n",
    "   Together, these approximate the effect of a full-rank update but require far fewer parameters.\n",
    "\n",
    "The second insight is motivated by the observation that **large overparameterized models have low intrinsic dimensionality**. In other words, their learned functions lie on a lower-dimensional subspace. This means we don’t need to update all dimensions to adapt the model effectively.\n",
    "\n",
    "Remarkably, the LoRA paper shows that the rank $r$ can be extremely small — even **rank-1** adaptations still produced meaningful improvements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gucYaPxQa84Y"
   },
   "source": [
    "## Do you need to implement LoRA from scratch?\n",
    "\n",
    "Not at all! While it’s a great learning exercise to implement LoRA from scratch (and we’ll walk through that in this post), you don’t have to reinvent the wheel. There are already excellent libraries that support LoRA and other PEFT methods.\n",
    "\n",
    "Some of the most popular options include:\n",
    "\n",
    "* **[Hugging Face PEFT](https://huggingface.co/docs/peft/index)** — integrates with the Hugging Face ecosystem and supports LoRA, adapters, and other techniques.\n",
    "* **[Adapters](https://adapterhub.ml)** — a standalone library focusing on adapter-based PEFT methods.\n",
    "\n",
    "These libraries make it easy to apply LoRA with just a few lines of code. In this blog post we will expore a from scratch implementation for the sake of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2_rBjICQ73F"
   },
   "source": [
    "### LoRA wrapper for `nn.Linear`\n",
    "\n",
    "This class takes a pretrained `nn.Linear` layer and augments it with a LoRA branch while keeping the original weights frozen. The frozen base layer, `self.base`, performs the usual linear transformation $y = W_0 x$. Instead of updating this weight matrix directly, LoRA adds a low-rank update on top.\n",
    "\n",
    "The update is built from two small matrices. The first, $A$, is a down-projection that reduces the input dimension $d$ to a much smaller rank $r$. The second, $B$, is an up-projection that maps back from $r$ to $d$. Together they approximate $\\Delta W$ as $B \\cdot A$. Because $r$ is typically very small, this drastically reduces the number of trainable parameters. The effect of the LoRA branch is further scaled by a factor of $\\alpha / r$, which controls how strongly the low-rank update influences the output.\n",
    "\n",
    "To ensure stability at the start of training, $A$ is initialized with Kaiming initialization so it starts with reasonable values, while $B$ is initialized with zeros so that the LoRA branch has no effect initially. An optional dropout is applied before the LoRA branch, matching the setup from the original paper.\n",
    "\n",
    "The forward pass therefore computes\n",
    "\n",
    "$$\n",
    "y = W_0 x + \\text{scaling} \\cdot (B(A(\\text{dropout}(x)))) ,\n",
    "$$\n",
    "\n",
    "where only the parameters of $A$ and $B$ require gradients. This makes training highly efficient, since the base layer remains frozen while the model is steered by a small number of additional parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"transformers>=4.41.0\" \"datasets>=2.19.0\" \"accelerate>=0.33.0\" \"evaluate>=0.4.2\" -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (AutoTokenizer, AutoConfig,\n",
    "                          DataCollatorWithPadding,\n",
    "                          Trainer, TrainingArguments,\n",
    "                          DistilBertForSequenceClassification)\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps an existing nn.Linear with LoRA: W x  +  scaling * (B @ A) x\n",
    "    - base_linear: the frozen nn.Linear to wrap\n",
    "    - r: LoRA rank\n",
    "    - alpha: LoRA scaling (effective scale = alpha / r)\n",
    "    - dropout: dropout on input to LoRA branch (like in original paper)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_linear: nn.Linear, r: int = 8, alpha: int = 16, dropout: float = 0.05):\n",
    "        super().__init__()\n",
    "        assert isinstance(base_linear, nn.Linear)\n",
    "        self.base = base_linear\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        self.lora_dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()\n",
    "        in_features = base_linear.in_features\n",
    "        out_features = base_linear.out_features\n",
    "\n",
    "        # LoRA matrices (A: down, B: up). Kaiming init for A, zeros for B per common practice.\n",
    "        self.A = nn.Linear(in_features, r, bias=False)\n",
    "        self.B = nn.Linear(r, out_features, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B.weight)\n",
    "\n",
    "        # Freeze base and ensure only LoRA params are trainable\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.base(x)\n",
    "        lora = self.B(self.A(self.lora_dropout(x))) * self.scaling\n",
    "        return y + lora\n",
    "\n",
    "    @property\n",
    "    def in_features(self):  # for compatibility if queried\n",
    "        return self.base.in_features\n",
    "\n",
    "    @property\n",
    "    def out_features(self):\n",
    "        return self.base.out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bczZZkKVcYG0"
   },
   "source": [
    "Next we get a model from `huggingface`. We chose `distilbert` because it is a small model that runs fast in Google Collab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, config=config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Mk1AjK8cmjF"
   },
   "source": [
    "We freeze the model except the output head (which is needed for the classification task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the whole model\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Make classifier head trainable\n",
    "for p in model.pre_classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7CQ2NtGSiui"
   },
   "source": [
    "## Wrapping the model\n",
    "Next we need to wrap our target weight matrices with our `LoRALinear` class. We stick to the papers implementation and wrap the query and value matrices of the attention blocks. We choose `r=8`, scaling `alpha=16` and `dropout = 0.05` which are pretty standart values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapped 12 attention linears with LoRA.\n"
     ]
    }
   ],
   "source": [
    "r = 8\n",
    "alpha = 16\n",
    "dropout = 0.05\n",
    "\n",
    "layers = model.distilbert.transformer.layer\n",
    "wrapped_count = 0\n",
    "for layer in layers:\n",
    "    attn = layer.attention\n",
    "    # q_lin\n",
    "    attn.q_lin = LoRALinear(attn.q_lin, r=r, alpha=alpha, dropout=dropout)\n",
    "    # v_lin\n",
    "    attn.v_lin = LoRALinear(attn.v_lin, r=r, alpha=alpha, dropout=dropout)\n",
    "    wrapped_count += 2\n",
    "\n",
    "print(f\"Wrapped {wrapped_count} attention linears with LoRA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 739,586 / 67,102,466 (1.10%)\n"
     ]
    }
   ],
   "source": [
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "pct = 100 * trainable / total\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA4XwfmfdexH"
   },
   "source": [
    "Next we load the dataset. We choses`sst2` from the `glue` benchmark suite. A pretty standart NLP benchmark. We need to tokenizer the dataset, set up a `DataCollator` for proper padding and truncation, and we restrict the dataset to 2000 samples so the training runs a little faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf0f28300b947db92eea104ce59c7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de78d0da17824772b5315e251d6f7811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f518703ca4a439aa92eed2d28b9ce25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "label_names = raw_dataset[\"train\"].features[\"label\"].names\n",
    "\n",
    "def preprocess(ex):\n",
    "    out = tokenizer(ex[\"sentence\"], truncation=True, max_length=256)\n",
    "    out[\"labels\"] = ex[\"label\"]\n",
    "    return out\n",
    "\n",
    "column_names = raw_dataset[\"train\"].column_names\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(preprocess, batched=True, remove_columns=column_names)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "# Optionally subsample to speed up:\n",
    "small_train = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(2000))   # ~2k samples\n",
    "small_val = tokenized_dataset[\"validation\"]  # full ~872"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRodPPcqevdw"
   },
   "source": [
    "We setup a Trainer class with our model and dataset and run ther training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-443219478.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.617900</td>\n",
       "      <td>0.425778</td>\n",
       "      <td>0.827982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.376600</td>\n",
       "      <td>0.375579</td>\n",
       "      <td>0.825688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.362300</td>\n",
       "      <td>0.368008</td>\n",
       "      <td>0.837156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.359925</td>\n",
       "      <td>0.842890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>0.355256</td>\n",
       "      <td>0.839450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.35525569319725037,\n",
       " 'eval_accuracy': 0.8394495412844036,\n",
       " 'eval_runtime': 0.6481,\n",
       " 'eval_samples_per_second': 1345.377,\n",
       " 'eval_steps_per_second': 43.2,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"lora-distilbert-sst2\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.0,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "lora_metrics = trainer.evaluate()\n",
    "lora_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egq9ZD_HerQ9"
   },
   "source": [
    "### Compared to Finetuning\n",
    "We load the same model from `huggingface` again.\n",
    "We initialize another trainer and retrain the model, this time without adding the LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2937415744.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.620600</td>\n",
       "      <td>0.458247</td>\n",
       "      <td>0.794725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.434931</td>\n",
       "      <td>0.801606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.661219</td>\n",
       "      <td>0.800459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>0.510780</td>\n",
       "      <td>0.827982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.497962</td>\n",
       "      <td>0.829128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4979621171951294,\n",
       " 'eval_accuracy': 0.8291284403669725,\n",
       " 'eval_runtime': 0.9443,\n",
       " 'eval_samples_per_second': 923.438,\n",
       " 'eval_steps_per_second': 29.652,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "ft_metrics = trainer.evaluate()\n",
    "ft_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1yls1D3XAII"
   },
   "source": [
    "comparing `eval_runtime`, `validation_loss` and `accuracies` give a clear picture. LoRA runs quicker,  generalizes fast and achive on par performance to full finetuning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
